{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# 加载分词器和模型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encode_text(text):\n",
    "    # 将文本编码为token IDs\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True, return_tensors='pt')\n",
    "    \n",
    "    # 使用BERT模型\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    \n",
    "    # 获取最后一层的隐藏状态\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    # 取[CLS] token的表示作为整个句子的表示\n",
    "    sentence_representation = last_hidden_state[:, 0, :]\n",
    "    return sentence_representation\n",
    "\n",
    "# 示例文本\n",
    "text = \"Here is an example of a normal-length text.\"\n",
    "representation = encode_text(text)\n",
    "\n",
    "print(representation.shape)  # 输出表示的形状\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User user1 has embeddings with shape: torch.Size([2, 768])\n",
      "User user2 has embeddings with shape: torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# 初始化tokenizer和model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 假设我们有一个字典，键为用户ID，值为该用户的帖子列表\n",
    "users_posts = {\n",
    "    \"user1\": [\"This is the first post content. Here are more details.\", \"Discussion about the second topic.\"],\n",
    "    \"user2\": [\"Here is another user's first post content.\", \"Here's the second post, including multiple viewpoints.\", \"Content of the third post.\"]\n",
    "}\n",
    "\n",
    "# 准备处理所有帖子\n",
    "all_embeddings = {}\n",
    "\n",
    "for user, posts in users_posts.items():\n",
    "    # 处理每个用户的帖子\n",
    "    processed_posts = [tokenizer.cls_token + \" \" + post for post in posts]\n",
    "    encoded_input = tokenizer(processed_posts, add_special_tokens=True, return_tensors='pt', padding=True, truncation=True, max_length=4096)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_input)\n",
    "    \n",
    "    # 提取每个帖子的[CLS]标记的输出作为帖子的嵌入表示\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    all_embeddings[user] = embeddings\n",
    "\n",
    "# 输出每个用户的帖子嵌入维度，以确保正确处理\n",
    "for user, embeddings in all_embeddings.items():\n",
    "    print(f\"User {user} has embeddings with shape: {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 对用户的帖子文本进行文本表示\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "# 初始化tokenizer和model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# 将模型移到GPU上\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "# 将reddit_json中的subreddit中set集合转化为列表\n",
    "def contents_elements(data_set):\n",
    "    # 验证数据类型\n",
    "    # print(f\"Data type: {type(data_set)}\")\n",
    "    # 如果是集合，则转换为列表并打印\n",
    "    if isinstance(data_set, set):\n",
    "        data_list = list(data_set)\n",
    "        return data_list\n",
    "    else:\n",
    "        print(\"Data is not a set.\")\n",
    "\n",
    "# 读取帖子pkl——json数据\n",
    "def read_posts_data(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        posts_data = pickle.load(f)\n",
    "    return posts_data\n",
    "\n",
    "\n",
    "def encode_posts(posts_data):\n",
    "    # all_embeddings = {}\n",
    "\n",
    "    user_labels = []\n",
    "    user_names = []\n",
    "\n",
    "    user_posts_embedings_dic = {\n",
    "        'user':[],\n",
    "        'embeddings':[],\n",
    "        'labels':[]\n",
    "    }\n",
    "    \n",
    "    data = read_posts_data(posts_data)\n",
    "    for i in range(len(data)):\n",
    "        user_posts  = []\n",
    "        user_labels.append(data[i]['label'])\n",
    "        user_names.append(data[i]['user'])\n",
    "        for j in range(len(data[i]['subreddit'])):\n",
    "            content = contents_elements(data[i]['subreddit'][j])[0]\n",
    "            content = content.lower()\n",
    "            user_posts.append(content)\n",
    "        processed_posts = [tokenizer.cls_token + \" \" + post for post in user_posts]\n",
    "\n",
    "        # print(processed_posts)\n",
    "\n",
    "        encoded_input = tokenizer(processed_posts, add_special_tokens=True, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded_input)\n",
    "\n",
    "        # 提取每个帖子的[CLS]标记的输出作为帖子的嵌入表示\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        # print(embeddings.shape)\n",
    "        # 一定要加这行代码不然，存储的文件会很大，因为embeddings是tensor类型\n",
    "        embeddings =  embeddings.cpu().numpy()\n",
    "\n",
    "        user_posts_embedings_dic['user'].append(user_names[i])\n",
    "        user_posts_embedings_dic['embeddings'].append(embeddings)\n",
    "        user_posts_embedings_dic['labels'].append(user_labels[i])\n",
    "\n",
    "        # print(user_posts_embedings_dic)\n",
    "        # break\n",
    "\n",
    "        # user_posts_embeddings_dic['user'] = user_names[i]\n",
    "        # user_posts_embeddings['label'] = user_labels[i]\n",
    "        # user_posts_embeddings['embeddings'] = embeddings\n",
    "\n",
    "        \n",
    "        # user_posts_embedings_dic[]\n",
    "        # user_posts_embedings.append(user_posts_embeddings)\n",
    "\n",
    "    # 将user_posts_embedings_dic转化为df对象\n",
    "    user_posts_embedings_df = pd.DataFrame(user_posts_embedings_dic)\n",
    "    # print(user_posts_embedings_df)\n",
    "\n",
    "\n",
    "    # 将得到的user_posts_embeddings写入pkl文件\n",
    "    with open('../data/bert_embeddings_csv.pkl', 'wb') as f:\n",
    "        pickle.dump(user_posts_embedings_df, f)\n",
    "\n",
    "\n",
    "posts_data = '../data/reddit_json.pkl'\n",
    "encode_posts(posts_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 768)\n"
     ]
    }
   ],
   "source": [
    "# 读取存储的帖子文本表示\n",
    "\n",
    "def read_emb_pkl(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        posts_data = pickle.load(f)\n",
    "    return posts_data\n",
    "\n",
    "def read_posts_embeddings(path):\n",
    "    data = read_emb_pkl(path)\n",
    "    # 读取其中embeddings的嵌入表示\n",
    "    for i in range(len(data)):\n",
    "        # print(data[i]['embeddings'])\n",
    "        if i == 1:\n",
    "            print(data[i]['embeddings'].shape)\n",
    "            break\n",
    "\n",
    "path = '../data/bert_embeddings.pkl'\n",
    "read_posts_embeddings(path= path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12673\n",
      "(1,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zxmheadway\\AppData\\Local\\Temp\\ipykernel_6852\\1419327849.py:36: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n",
      "  array_restored = np.fromstring(clean_str, dtype=int, sep=' ')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# 读取帖子文本表示\n",
    "def read_embeddings(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    return embeddings\n",
    "\n",
    "reddit_bert = '../data/bert_embeddings.pkl'\n",
    "embeddings = read_embeddings(reddit_bert)\n",
    "\n",
    "post_bert = []\n",
    "\n",
    "# 提取嵌入表示和标签\n",
    "for con in embeddings:\n",
    "    X = con['embeddings']\n",
    "    X = np.array2string(X)\n",
    "    post_bert.append(X)\n",
    "    # break\n",
    "# 将post_bert转化为PLK\n",
    "with open('../data/post_bert.pkl', 'wb') as f:\n",
    "    pickle.dump(post_bert, f)\n",
    "\n",
    "# 读取post_bert.pkl文件\n",
    "def read_post_bert(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        post_bert = pickle.load(f)\n",
    "    return post_bert\n",
    "\n",
    "post_bert = read_post_bert('../data/post_bert.pkl')\n",
    "print(len(post_bert[0])) \n",
    "\n",
    "clean_str = post_bert[0].strip('[]')\n",
    "array_restored = np.fromstring(clean_str, dtype=int, sep=' ')\n",
    "\n",
    "print(array_restored.shape)\n",
    "# df = pd.DataFrame(post_bert)\n",
    "\n",
    "# df.to_csv('../data/post_bert.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292\n"
     ]
    }
   ],
   "source": [
    "# 读取pkl文件\n",
    "\n",
    "import pickle\n",
    "\n",
    "def read_pkl(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "# 读取reddit_json.pkl文件\n",
    "reddit_json = '../data/bert_embeddings_csv.pkl'\n",
    "\n",
    "\n",
    "\n",
    "data = read_pkl(reddit_json)\n",
    "\n",
    "# 找到data中的embeddings的最大长度\n",
    "max_length = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    # print(len(data))\n",
    "    if len(data.iloc[i]['embeddings']) > max_length:\n",
    "        max_length = len(data.iloc[i]['embeddings'])\n",
    "        # print(max_length)\n",
    "        # break\n",
    "\n",
    "print(max_length)\n",
    "\n",
    "# print(data[0]['embeddings'].shape)\n",
    "\n",
    "\n",
    "# 找到reddit_json.pkl文件中的subreddit中的set集合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
