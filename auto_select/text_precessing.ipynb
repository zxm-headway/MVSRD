{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it not a viable option and youll be leaving your wife behind youd pain her beyond comprehensionit suck worrying about money i know that first hand it can definitely feel hopeless a you seem to be tired aware of your wife might need to chip in financially i know time is an issue but even 1015 hour a asthenia could alleviate a lot of the pressure in the meantime get your shit together write that resume tomorrow no excuse get it done and send it out whether you believe in some sort of powerful being or force governing thing or not thing really do work themselves out this is a big test for you and youll pull through just try to stay a positive a you can and everything will work out']\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pickle\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def string_list(post):\n",
    "    # 将每个post转换为list\n",
    "    converted_list = ast.literal_eval(post)\n",
    "    return converted_list\n",
    "\n",
    "# reddit = pd.read_csv('../data/reddit.csv')\n",
    "\n",
    "# # 将post列中的字符串转换为list，并且将不能转化成数据得到它的索引\n",
    "# error_index = []\n",
    "# for i in range(len(reddit)):\n",
    "#     try:\n",
    "#         string_list(reddit['Post'][i])\n",
    "#     except:\n",
    "#         error_index.append(i)\n",
    "\n",
    "# print(len(error_index))\n",
    "# print(error_index)\n",
    "\n",
    "\n",
    "def get_data_to_json():\n",
    "    reddit_500 = pd.read_csv('../data/reddit_500.csv')\n",
    "\n",
    "    # 将每个Post内容转化的list，并对每个元素进行JSON格式存储\n",
    "    reddit_json = []\n",
    "    for i in range(len(reddit_500)):\n",
    "        post_list = string_list(reddit_500['Post'][i])\n",
    "        post_json = {}\n",
    "        for post in post_list:\n",
    "            post_json['user'] = reddit_500['User'][i]\n",
    "            if 'subreddit' not in post_json.keys():\n",
    "                post_json['subreddit'] = [{post}]\n",
    "            else:\n",
    "                post_json['subreddit'].append({post})\n",
    "            post_json['label'] = reddit_500['Label'][i]\n",
    "        reddit_json.append(post_json)\n",
    "\n",
    "    return reddit_json\n",
    "\n",
    "\n",
    "# 读取pkl文件\n",
    "def read_pkl(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "# reddit_pkl = read_pkl('../data/reddit_json.pkl')\n",
    "\n",
    "# # reddit_pkl[268]['subreddit'].append({\"Honestly, that is not necessary to enjoy life. The only justifications to life is ising.  But this also means some arbitrary end isnt there either. Free yourself from that burden man. That is living for the past and living for the future and suffocating now by the enormity of it all, lift those shackles off of yourself. Life shouldnt be utter crap.  You feel like utter crap, and unfortunately the state of the condition is frequently thinking about ending it. Its a very catch 22 of affairs, its one of the chief symptoms and often why so many people die. The fact you have a very hard time geting this situation down to specifics feels like to me you are in a world of symptoms and we need a problem to deal with. Thing is, you started down this road 3 years ago. Seems abrupt to me.  You need to find a sensitive person or professional to hear your mess out.  Whatever it is, it could be anything, we are programmed to ignore it as a defence mechanism. Thats our immune system for these things and it frequently fucks with us. I kid you not, this could be as simple as a gastrointestinal issue, tons of things we dont understand about our body chemistry. People live good lives with the help of an [SSRI](http://en.wikipedia.org/wiki/Selective_serotonin_reuptake_inhibitor) which help to adjust an imbalance just like a person with diabeties needs insulin, and depression is often far more serious and goes too often untreated. We need to lift you up into the moment and into a cozy chair spilling your guts out, figuratively.\"})\n",
    "# # read_pkl1 = read_pkl\n",
    "# # reddit_pkl[268]['subreddit'].pop()\n",
    "\n",
    "# # reddit_pkl[497]['subreddit'][-1] = {\"Well failing is a part of life. Nothing would get done if everyone failed the first time and stop trying.Another true story: My english teacher assigned an essay (on a book I dont even care about). We had 2 weeks to do it. So like every other average student, I did it 11 pm the night before. I turned it in and he immediately gave it back saying to fix this and that. First I was like \\\"k\\\" and fix whatever he wanted to and gave it. He magically finds more mistakes and gives it back again. He accepts it \\\"for 30%\\\" AFTER 20 TIMES OF HANDING IT BACK AND FORTH. The last day, I literally got someone to fix it for me, and then got this same teacher to look at it again. Its was hella painful. But I did it. (Luckly, he was bluffing and gave me full credit)BTW: This still doesnt express the pure rage I had for this essay. There are just some words you shouldnt say because theyre inappropriate. But I was saying something under my breath. The point is people make mistakes and fail many MANY times. But as long as you keep trying your best, youll get it.If you learn your mistakes when you fail, you eventually succeed.As for music, let other people listen to it and comment on it. Just remember that its all constructive criticism and \\\"everyones a critic\\\". If you love doing it, keep going at it, no matter what anyone says.What kind of music do you make? \"}\n",
    "\n",
    "# print(reddit_pkl[497]['subreddit'][-1])\n",
    "\n",
    "# # 将现在的reddit_json存储为pkl文件\n",
    "# def save_pkl(data, file_path):\n",
    "#     with open(file_path, 'wb') as f:\n",
    "#         pickle.dump(data, f)\n",
    "\n",
    "# save_pkl(reddit_pkl, '../data/reddit_json.pkl')\n",
    "\n",
    "\n",
    "# 开始进行数据清洗工作\n",
    "\n",
    "# 读取pkl文件\n",
    "def read_pkl(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "# 将reddit_json中的subreddit中set集合转化为列表\n",
    "def contents_elements(data_set):\n",
    "    # 验证数据类型\n",
    "    # print(f\"Data type: {type(data_set)}\")\n",
    "    # 如果是集合，则转换为列表并打印\n",
    "    if isinstance(data_set, set):\n",
    "        data_list = list(data_set)\n",
    "        return data_list\n",
    "    else:\n",
    "        print(\"Data is not a set.\")\n",
    "\n",
    "\n",
    "\n",
    "# 定义文本清洗函数\n",
    "def cleaning(article):\n",
    "    punctuation = set(string.punctuation)\n",
    "    lemmatize = WordNetLemmatizer()\n",
    "    two = \"\".join(i for i in article if i not in punctuation)\n",
    "    three = \" \".join(lemmatize.lemmatize(i) for i in two.lower().split())\n",
    "    return three\n",
    "\n",
    "# 开始进行数据文本清洗\n",
    "\n",
    "def clean_posts():\n",
    "    reddit_pkl = read_pkl('../data/reddit_json.pkl')\n",
    "\n",
    "    # 遍历reddit_pkl中的subreddit，将其中的set集合转换为列表\n",
    "    for i in range(len(reddit_pkl)):\n",
    "        for j in range(len(reddit_pkl[i]['subreddit'])):\n",
    "            contents  = contents_elements(reddit_pkl[i]['subreddit'][j])\n",
    "            contents = cleaning(contents[0])\n",
    "            reddit_pkl[i]['subreddit'][j] = contents\n",
    "\n",
    "    # 将清洗后的数据存储为pkl文件\n",
    "    def save_pkl(data, file_path):\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    save_pkl(reddit_pkl, '../data/reddit_clean.pkl')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user-0': 132.0, 'user-1': 266.125, 'user-2': 117.25, 'user-3': 221.0, 'user-4': 33.666666666666664, 'user-5': 79.96296296296296, 'user-6': 36.333333333333336, 'user-7': 106.61111111111111, 'user-8': 623.6666666666666, 'user-9': 30.25, 'user-10': 75.0, 'user-11': 56.25, 'user-12': 32.333333333333336, 'user-13': 49.484848484848484, 'user-14': 160.5, 'user-15': 40.63636363636363, 'user-16': 66.88888888888889, 'user-17': 31.0, 'user-18': 128.0, 'user-19': 35.17857142857143, 'user-20': 136.5, 'user-21': 62.0, 'user-22': 100.5, 'user-23': 11.0, 'user-24': 48.476190476190474, 'user-25': 25.31578947368421, 'user-26': 47.41538461538462, 'user-27': 102.0, 'user-28': 185.08, 'user-29': 83.6, 'user-30': 106.5, 'user-31': 53.714285714285715, 'user-32': 67.77777777777777, 'user-33': 70.58823529411765, 'user-34': 74.5, 'user-35': 263.0, 'user-36': 319.8, 'user-37': 148.75, 'user-38': 142.66666666666666, 'user-39': 37.333333333333336, 'user-40': 225.0, 'user-41': 45.0, 'user-42': 90.5, 'user-43': 81.23333333333333, 'user-44': 41.5, 'user-45': 33.23529411764706, 'user-46': 125.8, 'user-47': 40.54545454545455, 'user-48': 135.28571428571428, 'user-49': 50.035714285714285, 'user-50': 90.0, 'user-51': 138.11111111111111, 'user-52': 75.0, 'user-53': 27.36842105263158, 'user-54': 109.0, 'user-55': 50.0, 'user-56': 36.8, 'user-57': 110.625, 'user-58': 21.5, 'user-59': 61.36666666666667, 'user-60': 58.34782608695652, 'user-61': 121.66666666666667, 'user-62': 115.5, 'user-63': 75.04166666666667, 'user-64': 60.0, 'user-65': 66.3529411764706, 'user-66': 14.5, 'user-67': 43.0, 'user-68': 118.0, 'user-69': 156.5, 'user-70': 60.75, 'user-71': 45.78947368421053, 'user-72': 74.75, 'user-73': 151.33333333333334, 'user-74': 437.0, 'user-75': 239.2, 'user-76': 198.7741935483871, 'user-77': 87.8840579710145, 'user-78': 212.5, 'user-79': 167.41666666666666, 'user-80': 23.8, 'user-81': 89.25, 'user-82': 39.0, 'user-83': 45.42857142857143, 'user-84': 54.111111111111114, 'user-85': 172.0, 'user-86': 145.85714285714286, 'user-87': 82.88461538461539, 'user-88': 33.5, 'user-89': 73.59183673469387, 'user-90': 28.833333333333332, 'user-91': 170.0, 'user-92': 30.076923076923077, 'user-93': 180.0, 'user-94': 150.54545454545453, 'user-95': 53.875, 'user-96': 42.357142857142854, 'user-97': 56.6, 'user-98': 44.588235294117645, 'user-99': 81.72, 'user-100': 154.5, 'user-101': 60.59090909090909, 'user-102': 193.1, 'user-103': 68.0, 'user-104': 16.0, 'user-105': 77.22222222222223, 'user-106': 73.0909090909091, 'user-107': 60.964285714285715, 'user-108': 62.666666666666664, 'user-109': 125.0, 'user-110': 74.28378378378379, 'user-111': 68.25, 'user-112': 95.0, 'user-113': 61.66326530612245, 'user-114': 72.45454545454545, 'user-115': 65.66666666666667, 'user-116': 100.6, 'user-117': 43.3768115942029, 'user-118': 101.33333333333333, 'user-119': 26.0, 'user-120': 180.5, 'user-121': 45.3, 'user-122': 215.66666666666666, 'user-123': 179.0, 'user-124': 45.09090909090909, 'user-125': 53.76851851851852, 'user-126': 70.13888888888889, 'user-127': 99.66666666666667, 'user-128': 95.7936507936508, 'user-129': 66.8, 'user-130': 92.75, 'user-131': 47.48936170212766, 'user-132': 48.0, 'user-133': 122.0, 'user-134': 272.0, 'user-135': 58.22222222222222, 'user-136': 36.0, 'user-137': 169.76, 'user-138': 50.07258064516129, 'user-139': 35.142857142857146, 'user-140': 74.0, 'user-141': 48.0, 'user-142': 70.37931034482759, 'user-143': 64.17647058823529, 'user-144': 50.61538461538461, 'user-145': 66.4, 'user-146': 183.57692307692307, 'user-147': 54.40196078431372, 'user-148': 37.258426966292134, 'user-149': 160.33333333333334, 'user-150': 113.16363636363636, 'user-151': 68.80392156862744, 'user-152': 134.0, 'user-153': 34.625, 'user-154': 30.458333333333332, 'user-155': 47.0, 'user-156': 518.3333333333334, 'user-157': 57.61904761904762, 'user-158': 21.0, 'user-159': 71.2, 'user-160': 20.0, 'user-161': 51.69047619047619, 'user-162': 68.95238095238095, 'user-163': 263.0, 'user-164': 55.410714285714285, 'user-165': 38.2, 'user-166': 140.0, 'user-167': 111.0, 'user-168': 50.0, 'user-169': 94.0, 'user-170': 160.0, 'user-171': 44.0, 'user-172': 20.333333333333332, 'user-173': 49.90909090909091, 'user-174': 156.09756097560975, 'user-175': 77.29032258064517, 'user-176': 68.35064935064935, 'user-177': 44.25, 'user-178': 131.92857142857142, 'user-179': 78.28571428571429, 'user-180': 92.45833333333333, 'user-181': 81.45454545454545, 'user-182': 162.6551724137931, 'user-183': 27.0, 'user-184': 27.916666666666668, 'user-185': 78.66666666666667, 'user-186': 127.72727272727273, 'user-187': 83.28571428571429, 'user-188': 30.0, 'user-189': 86.33802816901408, 'user-190': 194.85714285714286, 'user-191': 80.5, 'user-192': 29.5, 'user-193': 35.666666666666664, 'user-194': 20.285714285714285, 'user-195': 115.44444444444444, 'user-196': 37.27272727272727, 'user-197': 92.11111111111111, 'user-198': 70.0, 'user-199': 1101.0, 'user-200': 20.5, 'user-201': 30.142857142857142, 'user-202': 46.0, 'user-203': 74.7875, 'user-204': 211.0, 'user-205': 70.03448275862068, 'user-206': 100.27868852459017, 'user-207': 37.0, 'user-208': 23.5, 'user-209': 86.5, 'user-210': 22.097744360902254, 'user-211': 94.21428571428571, 'user-212': 61.958333333333336, 'user-213': 64.82857142857142, 'user-214': 176.8, 'user-215': 61.083333333333336, 'user-216': 52.0, 'user-217': 92.5, 'user-218': 89.61538461538461, 'user-219': 63.0, 'user-220': 109.20454545454545, 'user-221': 66.27586206896552, 'user-222': 56.37837837837838, 'user-223': 48.03225806451613, 'user-224': 98.15789473684211, 'user-225': 79.6, 'user-226': 65.85714285714286, 'user-227': 110.0, 'user-228': 199.0, 'user-229': 43.45, 'user-230': 31.25, 'user-231': 78.33333333333333, 'user-232': 207.0, 'user-233': 119.0, 'user-234': 55.666666666666664, 'user-235': 286.0, 'user-236': 231.5, 'user-237': 113.28571428571429, 'user-238': 244.25, 'user-239': 53.0, 'user-240': 36.0, 'user-241': 49.375, 'user-242': 128.62962962962962, 'user-243': 102.0, 'user-244': 142.0, 'user-245': 115.6470588235294, 'user-246': 161.45833333333334, 'user-247': 57.7047619047619, 'user-248': 67.5, 'user-249': 15.0, 'user-250': 60.82142857142857, 'user-251': 18.8, 'user-252': 31.75, 'user-253': 63.0, 'user-254': 79.88888888888889, 'user-255': 87.66101694915254, 'user-256': 189.0, 'user-257': 57.4, 'user-258': 74.0, 'user-259': 253.0, 'user-260': 111.6875, 'user-261': 58.25, 'user-262': 105.0, 'user-263': 45.75, 'user-264': 82.57142857142857, 'user-265': 96.22222222222223, 'user-266': 61.0, 'user-267': 82.04, 'user-268': 71.55, 'user-269': 291.0, 'user-270': 42.0, 'user-271': 29.5, 'user-272': 134.0, 'user-273': 124.0, 'user-274': 51.625, 'user-275': 46.5, 'user-276': 151.0, 'user-277': 134.0, 'user-278': 75.8, 'user-279': 216.0, 'user-280': 283.0, 'user-281': 326.0, 'user-282': 33.63636363636363, 'user-283': 108.71739130434783, 'user-284': 123.0, 'user-285': 170.9, 'user-286': 47.02857142857143, 'user-287': 31.6, 'user-288': 121.66666666666667, 'user-289': 44.5, 'user-290': 104.4, 'user-291': 35.833333333333336, 'user-292': 215.625, 'user-293': 73.24691358024691, 'user-294': 69.875, 'user-295': 47.1875, 'user-296': 73.5125, 'user-297': 108.25, 'user-298': 27.560606060606062, 'user-299': 78.0, 'user-300': 198.5, 'user-301': 56.666666666666664, 'user-302': 130.625, 'user-303': 369.3333333333333, 'user-304': 41.0, 'user-305': 24.72222222222222, 'user-306': 42.39473684210526, 'user-307': 126.8, 'user-308': 182.0, 'user-309': 73.125, 'user-310': 56.75, 'user-311': 36.26315789473684, 'user-312': 56.13461538461539, 'user-313': 115.14285714285714, 'user-314': 87.0, 'user-315': 19.0, 'user-316': 85.41666666666667, 'user-317': 74.51724137931035, 'user-318': 162.60714285714286, 'user-319': 309.25, 'user-320': 653.0, 'user-321': 368.0, 'user-322': 59.28, 'user-323': 93.36666666666666, 'user-324': 33.45161290322581, 'user-325': 154.28571428571428, 'user-326': 70.33333333333333, 'user-327': 73.0, 'user-328': 12.0, 'user-329': 125.5, 'user-330': 17.125, 'user-331': 132.5, 'user-332': 121.5, 'user-333': 74.2, 'user-334': 232.0, 'user-335': 120.57142857142857, 'user-336': 846.0, 'user-337': 120.0, 'user-338': 64.75, 'user-339': 28.6, 'user-340': 274.45454545454544, 'user-341': 63.5, 'user-342': 60.5, 'user-343': 188.3125, 'user-344': 18.94736842105263, 'user-345': 127.9, 'user-346': 228.93333333333334, 'user-347': 29.8, 'user-348': 33.48780487804878, 'user-349': 49.54736842105263, 'user-350': 145.625, 'user-351': 51.666666666666664, 'user-352': 43.6, 'user-353': 196.0, 'user-354': 48.357142857142854, 'user-355': 88.72222222222223, 'user-356': 70.92682926829268, 'user-357': 60.911764705882355, 'user-358': 36.75, 'user-359': 154.40540540540542, 'user-360': 402.0, 'user-361': 49.833333333333336, 'user-362': 13.0, 'user-363': 170.30769230769232, 'user-364': 182.66666666666666, 'user-365': 544.4166666666666, 'user-366': 69.625, 'user-367': 112.0, 'user-368': 240.25, 'user-369': 59.73913043478261, 'user-370': 140.42105263157896, 'user-371': 148.5, 'user-372': 101.0, 'user-373': 172.53846153846155, 'user-374': 154.0909090909091, 'user-375': 111.375, 'user-376': 103.66666666666667, 'user-377': 35.111111111111114, 'user-378': 25.666666666666668, 'user-379': 230.5, 'user-380': 40.55555555555556, 'user-381': 156.0, 'user-382': 68.75, 'user-383': 36.0, 'user-384': 215.71428571428572, 'user-385': 33.4, 'user-386': 174.0, 'user-387': 42.22222222222222, 'user-388': 84.33333333333333, 'user-389': 77.0625, 'user-390': 301.0, 'user-391': 34.5, 'user-392': 30.0, 'user-393': 90.5, 'user-394': 67.97142857142858, 'user-395': 70.0, 'user-396': 109.23076923076923, 'user-397': 11.571428571428571, 'user-398': 94.5, 'user-399': 90.0, 'user-400': 219.0, 'user-401': 198.83333333333334, 'user-402': 42.72222222222222, 'user-403': 206.0, 'user-404': 41.75, 'user-405': 180.25, 'user-406': 18.0, 'user-407': 132.9787234042553, 'user-408': 49.8, 'user-409': 97.28, 'user-410': 73.5, 'user-411': 65.65625, 'user-412': 110.35185185185185, 'user-413': 100.0, 'user-414': 173.5, 'user-415': 193.0, 'user-416': 72.92857142857143, 'user-417': 45.172413793103445, 'user-418': 170.5, 'user-419': 30.54794520547945, 'user-420': 69.76470588235294, 'user-421': 90.0, 'user-422': 62.0, 'user-423': 134.5, 'user-424': 124.88333333333334, 'user-425': 74.0, 'user-426': 59.75, 'user-427': 50.08695652173913, 'user-428': 81.58823529411765, 'user-429': 63.43617021276596, 'user-430': 46.23076923076923, 'user-431': 253.0, 'user-432': 229.0, 'user-433': 59.53333333333333, 'user-434': 32.75903614457831, 'user-435': 368.0, 'user-436': 285.5, 'user-437': 54.03125, 'user-438': 101.55357142857143, 'user-439': 61.25, 'user-440': 37.6, 'user-441': 199.77272727272728, 'user-442': 61.0, 'user-443': 72.93333333333334, 'user-444': 53.61702127659574, 'user-445': 35.26829268292683, 'user-446': 104.66666666666667, 'user-447': 568.0, 'user-448': 9.0, 'user-449': 147.75, 'user-450': 43.214285714285715, 'user-451': 47.63636363636363, 'user-452': 134.6, 'user-453': 58.5, 'user-454': 104.0, 'user-455': 42.333333333333336, 'user-456': 99.0909090909091, 'user-457': 105.0, 'user-458': 46.111111111111114, 'user-459': 301.0, 'user-460': 75.8, 'user-461': 80.13333333333334, 'user-462': 209.0, 'user-463': 143.5, 'user-464': 46.404761904761905, 'user-465': 130.1818181818182, 'user-466': 850.5, 'user-467': 149.05714285714285, 'user-468': 79.16666666666667, 'user-469': 156.5, 'user-470': 36.833333333333336, 'user-471': 56.333333333333336, 'user-472': 86.73684210526316, 'user-473': 284.90909090909093, 'user-474': 55.666666666666664, 'user-475': 24.0, 'user-476': 143.64285714285714, 'user-477': 152.57142857142858, 'user-478': 52.333333333333336, 'user-479': 79.0, 'user-480': 33.11764705882353, 'user-481': 24.17391304347826, 'user-482': 180.33333333333334, 'user-483': 28.0, 'user-484': 210.0, 'user-485': 47.1125, 'user-486': 276.3333333333333, 'user-487': 35.92307692307692, 'user-488': 98.0, 'user-489': 20.044520547945204, 'user-490': 61.56521739130435, 'user-491': 72.0, 'user-492': 73.26086956521739, 'user-493': 63.36363636363637, 'user-494': 64.0, 'user-495': 54.86363636363637, 'user-496': 64.57142857142857, 'user-497': 170.86486486486487, 'user-498': 87.2, 'user-499': 352.0}\n",
      "Total length: 53796.56475801899\n",
      "Average length: 107.59312951603798\n"
     ]
    }
   ],
   "source": [
    "# 计算每个用户的帖子数量\n",
    "\n",
    "def count_posts():\n",
    "    reddit_pkl = read_pkl('../data/reddit_clean.pkl')\n",
    "\n",
    "    # 计算每个用户的帖子数量\n",
    "    post_count = {}\n",
    "    for i in range(len(reddit_pkl)):\n",
    "        user = reddit_pkl[i]['user']\n",
    "        # print(user)\n",
    "        if user not in post_count.keys():\n",
    "            post_count[user] = len(reddit_pkl[i]['subreddit'])\n",
    "        else:\n",
    "            post_count[user] += 1\n",
    "\n",
    "    # print(post_count)\n",
    "\n",
    "    # 累计所有用户的帖子数量，计算平均帖子数量\n",
    "    total_post = 0\n",
    "    for key in post_count.keys():\n",
    "        total_post += post_count[key]\n",
    "    \n",
    "    average_post = total_post / len(post_count)\n",
    "    # print(f\"Total post: {total_post}\")\n",
    "    # print(f\"Average post: {average_post}\")\n",
    "\n",
    "    # # 找到帖子数量最大的用户\n",
    "    # max_post = max(post_count, key=post_count.get)\n",
    "    # print(f\"User with max post: {max_post}\")\n",
    "    # print(f\"Max post: {post_count[max_post]}\")\n",
    "\n",
    "\n",
    "    # 计算每个用户的帖子的平均长度\n",
    "    post_length = {}\n",
    "    for i in range(len(reddit_pkl)):\n",
    "        user = reddit_pkl[i]['user']\n",
    "        if user not in post_length.keys():\n",
    "            post_length[user] = 0\n",
    "            for j in range(len(reddit_pkl[i]['subreddit'])):\n",
    "                post_length[user] += len(reddit_pkl[i]['subreddit'][j].split())\n",
    "\n",
    "            post_length[user] = post_length[user] / len(reddit_pkl[i]['subreddit'])  \n",
    "\n",
    "\n",
    "    print(post_length)\n",
    "\n",
    "    # 计算每个用户的平均帖子长度\n",
    "    total_length = 0\n",
    "    for key in post_length.keys():\n",
    "        total_length += post_length[key]\n",
    "    \n",
    "    average_length = total_length / len(post_length)\n",
    "    print(f\"Total length: {total_length}\")\n",
    "    print(f\"Average length: {average_length}\")\n",
    "\n",
    "    # 将计算后的数据存储为pkl文件\n",
    "    def save_pkl(data, file_path):\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    # save_pkl(post_count, '../data/post_count.pkl')\n",
    "\n",
    "\n",
    "# clean_posts()\n",
    "# print(post_count)\n",
    "\n",
    "count_posts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user-0\n",
      "132.0\n",
      "user-1\n",
      "266.125\n",
      "user-8\n",
      "623.6666666666666\n",
      "user-199\n",
      "1101.0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 读取pkl数据清洗文件\n",
    "def read_pkl(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "# reddit_plk= read_pkl('../data/post_count.pkl')\n",
    "\n",
    "reddit_plk = read_pkl('../data/reddit_clean.pkl')\n",
    "\n",
    "# 得到最长的帖子\n",
    "# max_length = 0\n",
    "# for i in range(len(reddit_plk)):\n",
    "#     for j in range(len(reddit_plk[i]['subreddit'])):\n",
    "#         if len(reddit_plk[i]['subreddit'][j].split()) > max_length:\n",
    "#             max_length = len(reddit_plk[i]['subreddit'][j].split())\n",
    "#             # print(reddit_plk[i]['subreddit'][j])\n",
    "\n",
    "# print(max_length)\n",
    "\n",
    "\n",
    "# 每个的平均帖子长度\n",
    "post_length = {}\n",
    "for i in range(len(reddit_plk)):\n",
    "    user = reddit_plk[i]['user']\n",
    "    if user not in post_length.keys():\n",
    "        post_length[user] = 0\n",
    "        for j in range(len(reddit_plk[i]['subreddit'])):\n",
    "            post_length[user] += len(reddit_plk[i]['subreddit'][j].split())\n",
    "\n",
    "        post_length[user] = post_length[user] / len(reddit_plk[i]['subreddit'])\n",
    "\n",
    "# print(post_length)\n",
    "\n",
    "# 得到用户的最大平均帖子长度\n",
    "max_length = 0\n",
    "for key in post_length.keys():\n",
    "    if post_length[key] > max_length:\n",
    "        max_length = post_length[key]\n",
    "        print(key)\n",
    "        print(post_length[key])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# reddit_label = []\n",
    "\n",
    "# for i in range(len(reddit_plk)):\n",
    "#     reddit_label.append(reddit_plk[i]['label'])\n",
    "\n",
    "# print(reddit_label)\n",
    "# \n",
    "# print(len(reddit_label))\n",
    "\n",
    "\n",
    "# print(reddit_plk[0]['subreddit'])\n",
    "# reddit_label = read_pkl['label']\n",
    "# print(reddit_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it can be hard to appreciate the notion that you could meet someone else who will make you happy when you are so deeply in love with your boyfriend your desire are set on him and not much else will make you happy at the moment but over time the mind ha a way of dealing with loss this is a proven fact in psychology over time one day you will arrive at a level where you again feel at peace and can start feeling comfortable at looking into new relationshipsit is certainly uncomfortable dealing with your current situation but do understand that with some time and patience the pain will go away and you will get through the difficulty that many u student are encountering with debt unemployment and undervalued degreesthese are problem that many of u are facing right now you are not alone it is crushing to face all these difficulty at the same time but getting through them is what is going to make u stronger smarter and more emotionally aware than any generation that came before u the voice is just a voice people can praise you people can hate you but it doesnt change who you are think about it people can come up to you and say you are awesome but doe that change you in any way no it is a psychological thingthink about how absurd it is what you are saying here a little voice is telling you to kill yourself so youre actually going to do itif i went up to a random person and told them to kill themselves they would probably walk away from meyou should persevere you do so by not identifying with it it might always be there negative feeling and thought always present themselves to u when we are vulnerable but a voice is just a voice it is not u and we only suffer under it rule when we allow it an earthis happens to everyone it is just a matter of depth suicidal people have invested a lot of energy into i thought such a i am worthless i am miserable and you breathe life right into them by doing that and they come back to sap more life let them starveany insult against you is worthless unless you give it credit it is like a cheque that bounce at the bankso dont give into your negative thought and feeling stop giving them credit this is hard in the beginning and hard over time but you know you can actually be content and live a decent life if you take this seriously stop giving the i so much credit you indeed sound tired bipolar bipolar is when you take thing to such extreme it is a terrible illness i suffered it myself at one point and have taken lithium and a variety of other medication to deal with it it not so bad it help but ultimately the problem is a psychological one youre going to have to feel your way around your body and understand your own psychological profile your trigger and so forthwhat is also tired important to note that bipolarity is a two way street you might have these extreme moment of aversion the knife carving incident but what people dont realize is that you also have your moment of desire or what i mean by that is you go way too far into the thing you loveif you were just a bit more indifferent about thing youd stabilize a great deal i think having had the illness obviously stop taking drug and stuff like that if youre taking them but also stop staying up so late or doing whatever it is you do to an extreme live a balanced life bonus point if you find that life boring acclimate to that and make it your new norm a stable life is a happy life live a stable life where you are relatively indifferent to most thing you will enjoy your accumulated effort over the year and you will be pleased do not go into extreme of pain and pleasure which is essentially what you are doing leave well enough alone you will always come back so much stronger afterwards ive been cheated on in at least one or two long term relationship sometimes i dont know if the girl wa faithful in one of the relationship i digress it is extremely painful you will suffer for month on end but when you come out of it you will be better than you were before hardened by fire then you will meet somebody worth your time the person im going out with now is the best person ive ever gone out with by far the experience will prove that you are worth someone who is loyal and awesome you should never allow yourself the space to feel those feeling they are not some sort of natural feeling a people say they are any sort of feeling that started back when you were a teenager aught to be called into question we all have many stupid feeling a a teenager the problem is when those follow you into adulthoodi know people who are over 26 year old and engaging in the exact same habit that they were since they were 13 i mean thats just sad when are they going to grow up and move on why do people get stuck with such old sufferingslife is tired hard it is a constant improv but there are different way of looking at it not everything is bad just because youre suffering doesnt mean you need to take it so deeply to heart i mean come on you have a family that love you i avoid a lot of problem because i try not to focus on myself so much i think of what i can do for others this might sound egotistical in itself but really you have to understand the notion behind it when we think with our mind we always become tired miserable we never become happy through the way we think nothing is ever perfect or good when our mind ha a say in itbut when youre trying to help others it through the heart and it feel good to help them it make one happyi always recommend in these difficult time that people stop placing so much importance on their mind never allow negative thought inside your mind protect yourself with positive emotion if robber were pounding on your door would you open the door to them no but then suicidal thought come and they are just a bad a murderer and those who want to harm your wife and family why do you allow them spaceit is tired important for you to manage your mind if you have difficultly conjuring positivity in your interior world then you need to step back and question why that is what is holding you back from being a happy personquite often you will find it is these old friend these thought you identify with so much and feel to be so real it almost like they understand youwell you made them with your thought they subsist on your energy that they gain from you eating your food it entirely personalizedthat is why it is so difficult to stop identifying with the thought but you have to realize they are coming to you so theyre not you theyre just thought youre in control you dont need to feed into that destructive cycle force the negative thought out constantly demand that they stay out cultivate positivity love your wife and child do thing to make them happy absorb that inpositivity drive the negative thought out negative thought when accepted destroy the positivity it a real dragon and tiger scenario but you need to stick on the good side never give the enemy an inch theyll take a mile retire your old way of thinking it is a failed way it ha done you no good open your mind up embrace positivity have a revolution insist on embracing life well all die sooner or later but let have a part in bringing happiness to this planet while we can we need people like you to help u to come and help others you are strong youve made it this far but you made a mistake you leave the door open to that old enemy close it close it every time and bar it off you are not that thought of suicide i wa like you once i wa miserable and broken almost ended up with my life completely ruined i lived way too dangerously and almost lost my freedom ex long term girlfriend and everything i loved i turned my life around picked up the piece and built a new life out of it now my life is exciting i am talking to a beautiful new gal am progressing well in my study and everything is looking up do not identify with the down time in your life it always seems like it will never get better but thats not true eventually good thing do happen but we all have to do our part too dont be so hard on yourself man ive lived on my own for only like five month and im 27 it is extremely expensive to do that sort of thing nowadays back in the day you used to be able to build a house out of log and a guy your age could have 6 kid already nowadays many are lucky to buy a house by the time they are 50dont get all tied up in with this society it will burn you out it doesnt care about who you are inside it just care with what you can do mechanical laborbe ok with taking some time for yourself stop dating for a while be ok with taking some time to relax you are too wound up you arent going to find happiness by doing thing no matter how big the check list or how many thing you check off that doe not necessarily mean you will have lived a tired successful lifeinstead take some time to get to know yourself when you lay down to have a nap dont completely go to sleep just rock back and forth a little bit to barely stay away and just feel yourself in the bed just be aware you are there and keep doing that over and over again feeling will come to you and thought will come to you but you will become more aware of them just really feel them and get to understand themyou can change how everything is the reason you have all these girl cheat on you is because you cannot see them for how they are youre not able to see their real value youre not able to see who they are by just looking into their eyestake some time and read some philosophy from ancient greece appreciate some fine art read wikipediaits ok to take time if you just go out there again like a wild animal you will be in a wild chaotic abyss of angst fear and listlessness just be look at how you are thinking hereive never been good at anything i suck at school sport arteverything i have considered suicide on and off pretty much since i wa 13im 20 now i never had friend tired long they always decided they didnt like me and stopped talking to meeach of these thought is self enforcing they were created by you and fed by you and they will cease to exist when you stop feeding into them it is a negative thought cycleno self respecting person accepts negative thought negative emotion or negative people try to see the good in everything to watch nice thing and be around people who give you positive feelingsif your boyfriend is just sticking around for the money well then that doesnt sound tired positive doe it only you can know if he contributes positive value to your lifebut how foolish it is to assume that by going through the same psychological cycle that thing would improve life work in cycle everyones go to work go home and watch tv that is hardly unusual nowadaysthe difference is people can deal with that because they dont think negative thought about themselves you have to close the door to all negativity it is up to you when you open the door to negative emotion then it will sap you greatlyspend some time around your mother see the positive in your relationship with her she can teach you a lot but you have to be willing to learn \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 读取pkl文件\n",
    "def get_cleaned_data():\n",
    "    with open('../data/reddit_clean.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "reddit_label = []\n",
    "reddit_contents = []\n",
    "\n",
    "reddit_pkl = get_cleaned_data()\n",
    "for i in range(len(reddit_pkl)):\n",
    "    reddit_label.append(reddit_pkl[i]['label'])\n",
    "    # reddit_pkl[i]['label']内容进行拼接\n",
    "    contents = ''\n",
    "    for j in range(len(reddit_pkl[i]['subreddit'])):\n",
    "        contents += reddit_pkl[i]['subreddit'][j] + ' '\n",
    "    reddit_contents.append(contents)   \n",
    "\n",
    "print(reddit_contents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 定义一个按照顺序进行存储的集合\n",
    "\n",
    "from ordered_set import OrderedSet\n",
    "\n",
    "word = OrderedSet()\n",
    "\n",
    "world_list = ['a','m','d','ll','m','a']\n",
    "\n",
    "for i in world_list:\n",
    "    word.add(i)\n",
    "\n",
    "print(word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array 1 cleaned: [10, 20, 30, 40, 50]\n",
      "Array 1 average: 30.0\n",
      "Array 2 cleaned: [15, 25, 35, 45, 55]\n",
      "Array 2 average: 35.0\n"
     ]
    }
   ],
   "source": [
    "# 开始对社交媒体进行bert文本向量化实验\n",
    "import numpy as np\n",
    "\n",
    "def remove_outliers(arr):\n",
    "    mean = np.mean(arr)\n",
    "    std_dev = np.std(arr)\n",
    "    lower_bound = mean - 2 * std_dev  # 定义下界为平均值减两倍标准差\n",
    "    upper_bound = mean + 2 * std_dev  # 定义上界为平均值加两倍标准差\n",
    "    arr_without_outliers = [x for x in arr if x >= lower_bound and x <= upper_bound]\n",
    "    return arr_without_outliers\n",
    "\n",
    "def calculate_average(arr):\n",
    "    if len(arr) == 0:\n",
    "        return None\n",
    "    return sum(arr) / len(arr)\n",
    "\n",
    "# 两个示例数组\n",
    "array1 = [10, 20, 30, 40, 50, 200]  # 含有异常值200\n",
    "array2 = [15, 25, 35, 45, 55]\n",
    "\n",
    "# 剔除异常值\n",
    "array1_cleaned = remove_outliers(array1)\n",
    "array2_cleaned = remove_outliers(array2)\n",
    "\n",
    "# 计算平均值\n",
    "average1 = calculate_average(array1_cleaned)\n",
    "average2 = calculate_average(array2_cleaned)\n",
    "\n",
    "print(\"Array 1 cleaned:\", array1_cleaned)\n",
    "print(\"Array 1 average:\", average1)\n",
    "\n",
    "print(\"Array 2 cleaned:\", array2_cleaned)\n",
    "print(\"Array 2 average:\", average2)\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last hidden states shape: torch.Size([3, 9, 768])\n",
      "Transformer output shape: torch.Size([9, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, num_heads, dropout_rate):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_size * 4,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.transformer_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, embedded_sequence):\n",
    "        # 输入的embedded_sequence的形状为 [sequence_length, batch_size, embedding_size]\n",
    "        # 在Transformer中，序列的维度应该在第一个维度，所以我们需要将batch_size和sequence_length交换位置\n",
    "        embedded_sequence = embedded_sequence.permute(1, 0, 2)\n",
    "        \n",
    "        # 通过Transformer模型进行处理\n",
    "        transformer_output = self.transformer_encoder(embedded_sequence)\n",
    "        \n",
    "        return transformer_output\n",
    "\n",
    "# 示例帖子文本数据\n",
    "posts = [\n",
    "    \"This is the first post.\",\n",
    "    \"Another post about something interesting.\",\n",
    "    \"Yet another post with some content.\",\n",
    "]\n",
    "\n",
    "# 加载预训练的BERT模型和tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 编码帖子文本并获取嵌入表示\n",
    "encoded_inputs = tokenizer(posts, padding=True, truncation=True, return_tensors='pt')\n",
    "outputs = model(**encoded_inputs)\n",
    "\n",
    "# 获取BERT模型的最后一层隐藏状态（表示每个词的嵌入表示）\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "print(\"Last hidden states shape:\", last_hidden_states.shape)\n",
    "\n",
    "# 创建Transformer模型\n",
    "num_layers = 2  # Transformer中的层数\n",
    "hidden_size = 768  # Transformer中的隐藏层大小，与BERT模型输出的嵌入维度相同\n",
    "num_heads = 8  # 注意力头的数量\n",
    "dropout_rate = 0.1  # dropout率\n",
    "transformer_model = TransformerModel(num_layers, hidden_size, num_heads, dropout_rate)\n",
    "\n",
    "# 将嵌入表示输入到Transformer模型中进行处理\n",
    "transformer_output = transformer_model(last_hidden_states)\n",
    "\n",
    "print(\"Transformer output shape:\", transformer_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [2, 768] at entry 0 and [4, 768] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m     all_user_cls_representations\u001b[38;5;241m.\u001b[39mappend(cls_representation)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# 堆叠所有用户的CLS表示以形成用户序列\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m user_cls_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_user_cls_representations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# 创建用户编码器\u001b[39;00m\n\u001b[0;32m     63\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Transformer中的层数\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2, 768] at entry 0 and [4, 768] at entry 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "\n",
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, num_heads, dropout_rate):\n",
    "        super(UserEncoder, self).__init__()\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_size * 4,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.transformer_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, user_sequence):\n",
    "        # user_sequence的形状为 [batch_size, num_posts, embedding_size]\n",
    "        # 将用户帖子序列的维度交换以符合Transformer的输入要求\n",
    "        user_sequence = user_sequence.permute(1, 0, 2)\n",
    "        # 使用Transformer编码用户帖子序列\n",
    "        encoded_sequence = self.transformer_encoder(user_sequence)\n",
    "        return encoded_sequence\n",
    "\n",
    "# 示例用户及其帖子文本数据\n",
    "user_posts = {\n",
    "    \"User1\": [\n",
    "        \"This is the first post from User1.\",\n",
    "        \"Another post from User1 about something interesting.\",\n",
    "    ],\n",
    "    \"User2\": [\n",
    "        \"First post from User2.\",\n",
    "        \"Second post from User2.\",\n",
    "        \"Third post from User2 with some content.\",\n",
    "        \"And one more post from User2.\",\n",
    "    ],\n",
    "    \"User3\": [\n",
    "        \"Only one post from User3.\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 加载预训练的BERT模型和tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 对每个用户的帖子进行编码并获取嵌入表示\n",
    "all_user_cls_representations = []\n",
    "for user, posts in user_posts.items():\n",
    "    encoded_inputs = tokenizer(posts, padding=True, truncation=True, return_tensors='pt')\n",
    "    outputs = model(**encoded_inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    # 提取CLS token的表示\n",
    "    cls_representation = last_hidden_states[:, 0, :]\n",
    "    all_user_cls_representations.append(cls_representation)\n",
    "\n",
    "# 堆叠所有用户的CLS表示以形成用户序列\n",
    "user_cls_sequence = torch.stack(all_user_cls_representations)\n",
    "\n",
    "# 创建用户编码器\n",
    "num_layers = 2  # Transformer中的层数\n",
    "hidden_size = 768  # Transformer中的隐藏层大小，与BERT模型输出的嵌入维度相同\n",
    "num_heads = 8  # 注意力头的数量\n",
    "dropout_rate = 0.1  # dropout率\n",
    "user_encoder = UserEncoder(num_layers, hidden_size, num_heads, dropout_rate)\n",
    "\n",
    "# 将用户帖子序列传递给用户编码器进行编码\n",
    "encoded_user_sequence = user_encoder(user_cls_sequence)\n",
    "\n",
    "print(\"Encoded user sequence shape:\", encoded_user_sequence.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1\n",
      " 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 1\n",
      " 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0\n",
      " 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1]\n",
      "Class counts: {0: 100, 1: 100}\n",
      "Train class counts: {0: 80, 1: 80}\n",
      "Train class counts: {0: 80, 1: 80}\n",
      "Train class counts: {0: 80, 1: 80}\n",
      "Train class counts: {0: 80, 1: 80}\n",
      "Train class counts: {0: 80, 1: 80}\n",
      "每次折叠的准确度: [0.925, 0.8, 0.85, 0.875, 0.9]\n",
      "平均准确度: 0.8700000000000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 生成模拟数据，确保每个类别都有足够的样本\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.5, 0.5], flip_y=0, random_state=1)\n",
    "\n",
    "print(y)\n",
    "\n",
    "# 计算每个类别的样本比列\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "print(\"Class counts:\", class_counts)\n",
    "\n",
    "\n",
    "\n",
    "# 初始化StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 初始化分类器\n",
    "model = LogisticRegression()\n",
    "\n",
    "# 存储每次迭代的准确度\n",
    "accuracies = []\n",
    "\n",
    "# 使用StratifiedKFold\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # print(\"Train set:\", y_train)\n",
    "\n",
    "    # 计算训练集中每个类别的样本比例\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    class_counts = dict(zip(unique, counts))\n",
    "    print(\"Train class counts:\", class_counts)\n",
    "\n",
    "\n",
    "\n",
    "    # 检查训练集中是否至少包含两个类别\n",
    "    if len(np.unique(y_train)) < 2:\n",
    "        continue\n",
    "\n",
    "    # 训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 预测\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 计算准确度\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# 输出结果\n",
    "print(\"每次折叠的准确度:\", accuracies)\n",
    "print(\"平均准确度:\", np.mean(accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Class distribution: tensor([ 4, 12])\n",
      "Batch 1, Class distribution: tensor([8, 8])\n",
      "Batch 2, Class distribution: tensor([8, 8])\n",
      "Batch 3, Class distribution: tensor([ 6, 10])\n",
      "Batch 4, Class distribution: tensor([11,  5])\n",
      "Batch 5, Class distribution: tensor([9, 7])\n",
      "Batch 6, Class distribution: tensor([1, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, TensorDataset\n",
    "\n",
    "# 假设 X 和 y 已经是 PyTorch 张量，且已经进行了适当的预处理\n",
    "X = torch.randn(200, 3)  # 100 个样本，每个样本 2 个特征\n",
    "y = torch.cat((torch.zeros(90), torch.ones(30),))  # 极度不平衡的数据集\n",
    "y = y.long()  # 将标签转换为整数类型\n",
    "\n",
    "# 计算每个类的权重\n",
    "class_counts = torch.tensor([(y == t).sum() for t in torch.unique(y)])\n",
    "class_weights = 1. / class_counts.float()  # 取倒数得到权重\n",
    "weights = class_weights[y.long()]  # 每个样本的权重\n",
    "\n",
    "# 创建 WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "# 创建数据集\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "# 创建 DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=16, sampler=sampler)\n",
    "\n",
    "# 遍历 DataLoader\n",
    "for i, (inputs, targets) in enumerate(dataloader):\n",
    "    print(f\"Batch {i}, Class distribution: {torch.bincount(targets)}\")\n",
    "    # 这里可以添加模型训练代码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
      "        0.0100, 0.0100, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059,\n",
      "        0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059,\n",
      "        0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059,\n",
      "        0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059,\n",
      "        0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059,\n",
      "        0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059,\n",
      "        0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059,\n",
      "        0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059,\n",
      "        0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059,\n",
      "        0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059,\n",
      "        0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059,\n",
      "        0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059,\n",
      "        0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059,\n",
      "        0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059,\n",
      "        0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059,\n",
      "        0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059,\n",
      "        0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059,\n",
      "        0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059,\n",
      "        0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059, 0.0059,\n",
      "        0.0059, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133,\n",
      "        0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133,\n",
      "        0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133,\n",
      "        0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133,\n",
      "        0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133,\n",
      "        0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133,\n",
      "        0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133,\n",
      "        0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0133,\n",
      "        0.0133, 0.0133, 0.0133, 0.0133, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
      "        0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
      "        0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
      "        0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
      "        0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222,\n",
      "        0.0222, 0.0222, 0.0222, 0.0222])\n",
      "Batch 1:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([2, 2, 1, 3, 4, 3, 4, 2, 2, 2, 0, 3, 1, 0, 4, 3])\n",
      "Class counts: {0: 2, 1: 2, 2: 5, 3: 4, 4: 3}\n",
      "Batch 2:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([4, 3, 4, 2, 2, 4, 1, 2, 0, 0, 3, 4, 2, 2, 2, 3])\n",
      "Class counts: {0: 2, 1: 1, 2: 6, 3: 3, 4: 4}\n",
      "Batch 3:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([0, 1, 3, 2, 0, 4, 3, 2, 2, 3, 3, 2, 4, 1, 3, 2])\n",
      "Class counts: {0: 2, 1: 2, 2: 5, 3: 5, 4: 2}\n",
      "Batch 4:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([3, 4, 3, 4, 1, 4, 3, 1, 0, 0, 1, 4, 4, 0, 2, 2])\n",
      "Class counts: {0: 3, 1: 3, 2: 2, 3: 3, 4: 5}\n",
      "Batch 5:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([3, 1, 0, 4, 4, 1, 1, 0, 3, 2, 3, 2, 1, 1, 2, 4])\n",
      "Class counts: {0: 2, 1: 5, 2: 3, 3: 3, 4: 3}\n",
      "Batch 6:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([4, 0, 0, 0, 2, 2, 1, 2, 2, 3, 4, 4, 4, 0, 1, 1])\n",
      "Class counts: {0: 4, 1: 3, 2: 4, 3: 1, 4: 4}\n",
      "Batch 7:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([0, 0, 0, 3, 1, 1, 3, 2, 3, 4, 2, 4, 1, 2, 1, 3])\n",
      "Class counts: {0: 3, 1: 4, 2: 3, 3: 4, 4: 2}\n",
      "Batch 8:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([4, 2, 0, 3, 0, 3, 4, 2, 3, 0, 3, 0, 0, 4, 2, 3])\n",
      "Class counts: {0: 5, 2: 3, 3: 5, 4: 3}\n",
      "Batch 9:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([0, 4, 3, 3, 2, 3, 2, 4, 3, 1, 1, 2, 1, 1, 2, 2])\n",
      "Class counts: {0: 1, 1: 4, 2: 5, 3: 4, 4: 2}\n",
      "Batch 10:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([1, 0, 1, 4, 1, 2, 0, 2, 4, 4, 1, 0, 3, 0, 3, 4])\n",
      "Class counts: {0: 4, 1: 4, 2: 2, 3: 2, 4: 4}\n",
      "Batch 11:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([4, 3, 1, 1, 1, 2, 1, 4, 2, 4, 4, 2, 3, 1, 0, 3])\n",
      "Class counts: {0: 1, 1: 5, 2: 3, 3: 3, 4: 4}\n",
      "Batch 12:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([1, 2, 1, 0, 3, 4, 0, 0, 0, 3, 2, 2, 4, 0, 0, 0])\n",
      "Class counts: {0: 7, 1: 2, 2: 3, 3: 2, 4: 2}\n",
      "Batch 13:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([0, 3, 4, 2, 1, 3, 3, 1, 2, 3, 0, 2, 1, 4, 2, 2])\n",
      "Class counts: {0: 2, 1: 3, 2: 5, 3: 4, 4: 2}\n",
      "Batch 14:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([0, 3, 0, 2, 1, 3, 2, 0, 1, 4, 2, 4, 0, 2, 0, 3])\n",
      "Class counts: {0: 5, 1: 2, 2: 4, 3: 3, 4: 2}\n",
      "Batch 15:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([3, 4, 2, 3, 2, 4, 3, 4, 2, 1, 0, 0, 1, 1, 4, 1])\n",
      "Class counts: {0: 2, 1: 4, 2: 3, 3: 3, 4: 4}\n",
      "Batch 16:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([2, 1, 4, 0, 4, 4, 2, 3, 3, 4, 1, 1, 3, 4, 3, 0])\n",
      "Class counts: {0: 2, 1: 3, 2: 2, 3: 4, 4: 5}\n",
      "Batch 17:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([1, 3, 3, 0, 0, 0, 1, 0, 1, 3, 3, 0, 1, 2, 2, 3])\n",
      "Class counts: {0: 5, 1: 4, 2: 2, 3: 5}\n",
      "Batch 18:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([4, 1, 1, 2, 4, 0, 1, 0, 0, 1, 1, 3, 4, 1, 4, 2])\n",
      "Class counts: {0: 3, 1: 6, 2: 2, 3: 1, 4: 4}\n",
      "Batch 19:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([1, 3, 3, 1, 1, 2, 4, 3, 1, 2, 1, 1, 2, 0, 3, 0])\n",
      "Class counts: {0: 2, 1: 6, 2: 3, 3: 4, 4: 1}\n",
      "Batch 20:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([2, 0, 0, 1, 3, 3, 3, 4, 2, 4, 4, 1, 3, 1, 4, 4])\n",
      "Class counts: {0: 2, 1: 3, 2: 2, 3: 4, 4: 5}\n",
      "Batch 21:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([3, 1, 1, 0, 2, 2, 4, 4, 4, 1, 3, 4, 1, 4, 2, 1])\n",
      "Class counts: {0: 1, 1: 5, 2: 3, 3: 2, 4: 5}\n",
      "Batch 22:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([3, 3, 3, 2, 4, 4, 4, 4, 4, 2, 2, 1, 2, 3, 2, 0])\n",
      "Class counts: {0: 1, 1: 1, 2: 5, 3: 4, 4: 5}\n",
      "Batch 23:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([4, 1, 0, 2, 4, 3, 0, 0, 2, 0, 3, 1, 4, 1, 3, 4])\n",
      "Class counts: {0: 4, 1: 3, 2: 2, 3: 3, 4: 4}\n",
      "Batch 24:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([0, 4, 1, 3, 1, 4, 2, 1, 3, 3, 2, 2, 2, 2, 1, 4])\n",
      "Class counts: {0: 1, 1: 4, 2: 5, 3: 3, 4: 3}\n",
      "Batch 25:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([2, 2, 0, 1, 4, 4, 3, 3, 0, 1, 4, 3, 2, 4, 0, 2])\n",
      "Class counts: {0: 3, 1: 2, 2: 4, 3: 3, 4: 4}\n",
      "Batch 26:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([4, 3, 4, 2, 0, 2, 1, 2, 1, 4, 0, 4, 4, 2, 3, 4])\n",
      "Class counts: {0: 2, 1: 2, 2: 4, 3: 2, 4: 6}\n",
      "Batch 27:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([2, 4, 2, 2, 1, 4, 4, 0, 2, 4, 2, 4, 1, 2, 2, 0])\n",
      "Class counts: {0: 2, 1: 2, 2: 7, 4: 5}\n",
      "Batch 28:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([2, 0, 3, 3, 2, 2, 1, 4, 1, 3, 1, 3, 0, 2, 1, 0])\n",
      "Class counts: {0: 3, 1: 4, 2: 4, 3: 4, 4: 1}\n",
      "Batch 29:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([4, 3, 4, 4, 0, 2, 4, 1, 4, 0, 3, 1, 4, 0, 3, 1])\n",
      "Class counts: {0: 3, 1: 3, 2: 1, 3: 3, 4: 6}\n",
      "Batch 30:\n",
      "Data shape: torch.Size([16, 10])\n",
      "Labels: tensor([0, 1, 0, 2, 0, 1, 1, 3, 0, 4, 2, 0, 2, 0, 0, 4])\n",
      "Class counts: {0: 7, 1: 3, 2: 3, 3: 1, 4: 2}\n",
      "Batch 31:\n",
      "Data shape: torch.Size([10, 10])\n",
      "Labels: tensor([0, 3, 1, 2, 1, 4, 3, 3, 3, 2])\n",
      "Class counts: {0: 1, 1: 2, 2: 2, 3: 4, 4: 1}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "# 定义一个自定义数据集\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        :param data: 包含样本特征的张量\n",
    "        :param labels: 包含样本标签的张量\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回数据集中的样本数\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        根据给定的索引idx返回样本\n",
    "        :param idx: 样本的索引\n",
    "        \"\"\"\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# 生成随机数据和标签\n",
    "data = torch.randn(500, 10)  # 假设每个样本有10个特征\n",
    "labels = torch.cat([\n",
    "    torch.zeros(100, dtype=torch.long),  # 类别0\n",
    "    torch.ones(100, dtype=torch.long),   # 类别1\n",
    "    torch.full((170,), 2, dtype=torch.long),  # 类别2\n",
    "    torch.full((75,), 3, dtype=torch.long),   # 类别3\n",
    "    torch.full((45,), 4, dtype=torch.long)    # 类别4\n",
    "])\n",
    "\n",
    "# 计算每个类别的权重\n",
    "weights = 1. / torch.tensor([108, 99, 171, 77, 45], dtype=torch.float)\n",
    "samples_weights = weights[labels]\n",
    "\n",
    "# 创建WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(samples_weights, num_samples=len(samples_weights), replacement=True)\n",
    "\n",
    "# 实例化数据集\n",
    "dataset = CustomDataset(data, labels)\n",
    "\n",
    "# 创建DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=16, sampler=sampler)\n",
    "\n",
    "# 使用DataLoader进行训练\n",
    "for batch_idx, (data, target) in enumerate(data_loader):\n",
    "    # 这里可以添加模型训练的代码\n",
    "    print(f'Batch {batch_idx + 1}:')\n",
    "    print('Data shape:', data.shape)  # 打印数据的形状\n",
    "    print('Labels:', target)          # 打印标签\n",
    "\n",
    "\n",
    "    # 计算每个类别的样本比例\n",
    "    unique, counts = torch.unique(target, return_counts=True)\n",
    "    class_counts = dict(zip(unique.numpy(), counts.numpy()))\n",
    "\n",
    "    print(\"Class counts:\", class_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
